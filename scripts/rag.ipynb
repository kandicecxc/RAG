{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "To facilitate Data Science Application (DSA) subject (IAAust) knowledge integration, a Retrieval-Augmented Generation (RAG) approach can be used together with a LLM.  \n",
    "Different retrieval methods were compared and the Maximum Marginal Relevance method was found to work best in retrieving diverse and unique results from vectorstore.  \n",
    "Running a few questions to test showed encouraging answers - the LLM was pulling correct answers from relevant documents and summarizing them in a succinct manner.\n",
    "\n",
    "Basic workflow: Financial text -> Split into chunks + OpenAI embeddings -> Load Vectorstore  \n",
    "Ask questions -> Retrieval from Vectorstore -> Get most relevant embeddings -> Expose to LLM -> Get relevant answer\n",
    "\n",
    "Some possible follow-up that extends beyond the scope of this project:\n",
    "* Can this workflow scale? Three documents were used but how about a hundred?\n",
    "* Chatbot functionality with memory and a clean interface can be built for non-technical stakeholders\n",
    "* How can model performance drift be tracked for LLMs? What if quality of embeddings/results deteriorate over time?\n",
    "* Can tabular data be created by specifically specifying the output? This could serve as input to ML/DL predictive models.\n",
    "\n",
    "Inspired by: https://learn.deeplearning.ai/langchain-chat-with-your-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\caixi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import GPT4All\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.retrievers.svm import SVMRetriever\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import nltk\n",
    "from google import genai\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Try to parse the JSON response\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"JSONDecodeError: Could not parse response as JSON\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "    \n",
    "    # If there was an error, sleep a bit and return None\n",
    "    sleep(1)\n",
    "    return None\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "headers = {\"Authorization\": \"Bearer <API_KEY>\"}  # Free tokens available at huggingface.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an appropriate text splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"../data/DSA 2025 C07 Natural Language Processing_1.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fellowship Program \\n  \\nData Science Applications \\nChapter 7: Natural Language \\nProcessing'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Adobe PDF Services',\n",
       " 'creator': 'Microsoft® Word 2019',\n",
       " 'creationdate': '2024-12-18T04:48:59+00:00',\n",
       " 'source': '../data/DSA 2025 C07 Natural Language Processing_1.pdf',\n",
       " 'file_path': '../data/DSA 2025 C07 Natural Language Processing_1.pdf',\n",
       " 'total_pages': 66,\n",
       " 'format': 'PDF 1.7',\n",
       " 'title': 'Data Science Applications',\n",
       " 'author': 'User',\n",
       " 'subject': 'Natural Language Processing',\n",
       " 'keywords': 'Chapter 7',\n",
       " 'moddate': '2024-12-18T04:49:02+00:00',\n",
       " 'trapped': '',\n",
       " 'modDate': 'D:20241218044902Z',\n",
       " 'creationDate': 'D:20241218044859Z',\n",
       " 'page': 50}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[50].metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between text splitters\n",
    "\n",
    "Let's choose which TextSplitter to use. Here I'll compare results between `RecursiveCharacterTextSplitter` and `NLTKTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small paramaters for now to conveniently assess results\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # default values\n",
    ")\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# random page\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m snippet \u001b[38;5;241m=\u001b[39m \u001b[43mpages\u001b[49m[\u001b[38;5;241m25\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3000\u001b[39m]\n\u001b[0;32m      3\u001b[0m snippet\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "# random page\n",
    "snippet = pages[25].page_content[0:3000]\n",
    "snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_res = r_splitter.split_text(snippet)\n",
    "len(r_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Data Science Applications \\nChapter 7: Natural Language Processing \\n \\n \\n \\n \\n \\n \\n \\n© December 2024 The Institute of Actuaries of Australia \\nPage 26 of 66 \\nBoW or TF-IDF if handling sparse, high-dimensional data isn't an issue, or if simplicity is \\nprioritized. \\n• \\nImportance of Word Frequency: Use TF-IDF if differentiating between commonly-occurring \\nand rare terms is important (e.g., information retrieval). Use BoW if simple word occurrence or\",\n",
       " 'frequency is sufficient. Use Word Embeddings for deeper semantic and context-based \\nanalysis where word frequency is less relevant. \\n• \\nText Size and Dataset Complexity: Use BoW or TF-IDF for simpler, smaller text datasets. Use \\nWord Embeddings for larger datasets with complex relationships between words. \\n• \\nComputational Complexity: Use BoW or TF-IDF if computational resources are limited or if the \\ntask requires quick and easy implementation. Use Word Embeddings if computational',\n",
       " \"resources and time are available for training, or using pre-trained models. \\n• \\nInterpretability: Use BoW or TF-IDF if interpretability is important (e.g., explaining models to \\nnon-technical stakeholders). Use Word Embeddings if interpretability is less of a concern and \\ndeeper representation learning is prioritized. \\n• \\nContext Sensitivity: Use Word Embeddings for tasks where context is crucial. Use BoW or TF-\\nIDF for simpler tasks where word context isn't as important.\",\n",
       " '7.2.5. Model Building on Vectorised Text \\nFinally, optimal results are often achieved by combining multiple vectorisation methods. For \\nexample, in search algorithms, it can be helpful to match both the semantic meaning and a subset \\nof the exact words. Run data science algorithm. \\nOnce the steps outlined in Sections 7.2.1–7.2.4 have been followed, you will have a dataset of \\nnumbers that represent the text. Data science techniques, such as those covered in Chapter 5',\n",
       " '(Classification) and Chapter 6 (Unsupervised learning), can then be applied to the vectorised text. \\nAn example of applying supervised learning techniques in NLP is the classification of text that has \\nlabelled responses. For example, the response variable might indicate whether a piece of writing, \\nsuch as a business review, is positive, negative or neutral. Supervised learning techniques such \\nas neural networks can be trained on the vectorised text and labelled responses to predict whether',\n",
       " 'another unseen text is positive, negative or neutral. Specific uses of supervised machine learning \\nin NLP are discussed further in Section 7.4. \\nUnsupervised machine learning involves training a model on data that does not contain labelled \\nresponses. An example of using unsupervised training in NLP is the use of clustering to group \\nsimilar documents. This is commonly referred to as ‘topic modelling’. This method is particularly']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_res = nltk_splitter.split_text(snippet)\n",
    "len(nltk_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Data Science Applications \\nChapter 7: Natural Language Processing \\n \\n \\n \\n \\n \\n \\n \\n© December 2024 The Institute of Actuaries of Australia \\nPage 26 of 66 \\nBoW or TF-IDF if handling sparse, high-dimensional data isn't an issue, or if simplicity is \\nprioritized.\\n\\n• \\nImportance of Word Frequency: Use TF-IDF if differentiating between commonly-occurring \\nand rare terms is important (e.g., information retrieval).\\n\\nUse BoW if simple word occurrence or \\nfrequency is sufficient.\",\n",
       " 'Use Word Embeddings for deeper semantic and context-based \\nanalysis where word frequency is less relevant.\\n\\n• \\nText Size and Dataset Complexity: Use BoW or TF-IDF for simpler, smaller text datasets.\\n\\nUse \\nWord Embeddings for larger datasets with complex relationships between words.\\n\\n• \\nComputational Complexity: Use BoW or TF-IDF if computational resources are limited or if the \\ntask requires quick and easy implementation.',\n",
       " 'Use Word Embeddings if computational \\nresources and time are available for training, or using pre-trained models.\\n\\n• \\nInterpretability: Use BoW or TF-IDF if interpretability is important (e.g., explaining models to \\nnon-technical stakeholders).\\n\\nUse Word Embeddings if interpretability is less of a concern and \\ndeeper representation learning is prioritized.\\n\\n• \\nContext Sensitivity: Use Word Embeddings for tasks where context is crucial.',\n",
       " \"Use BoW or TF-\\nIDF for simpler tasks where word context isn't as important.\\n\\n7.2.5.\\n\\nModel Building on Vectorised Text \\nFinally, optimal results are often achieved by combining multiple vectorisation methods.\\n\\nFor \\nexample, in search algorithms, it can be helpful to match both the semantic meaning and a subset \\nof the exact words.\\n\\nRun data science algorithm.\\n\\nOnce the steps outlined in Sections 7.2.1–7.2.4 have been followed, you will have a dataset of \\nnumbers that represent the text.\",\n",
       " 'Data science techniques, such as those covered in Chapter 5 \\n(Classification) and Chapter 6 (Unsupervised learning), can then be applied to the vectorised text.\\n\\nAn example of applying supervised learning techniques in NLP is the classification of text that has \\nlabelled responses.\\n\\nFor example, the response variable might indicate whether a piece of writing, \\nsuch as a business review, is positive, negative or neutral.',\n",
       " 'Supervised learning techniques such \\nas neural networks can be trained on the vectorised text and labelled responses to predict whether \\nanother unseen text is positive, negative or neutral.\\n\\nSpecific uses of supervised machine learning \\nin NLP are discussed further in Section 7.4.\\n\\nUnsupervised machine learning involves training a model on data that does not contain labelled \\nresponses.\\n\\nAn example of using unsupervised training in NLP is the use of clustering to group \\nsimilar documents.',\n",
       " 'This is commonly referred to as ‘topic modelling’.\\n\\nThis method is particularly']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ending of each split chunk by `RecursiveCharacterTextSplitter` doesn't coincide with the end of a sentence/puncutuation, whereas `NLTKTextSplitter` captures that nuance.  \n",
    "Let's go with `NLTKTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split PDF and assess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1584, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "chunk_overlap = chunk_size * 0.1\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_res = nltk_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Services', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-12-18T04:48:59+00:00', 'source': '../data/DSA 2025 C07 Natural Language Processing_1.pdf', 'file_path': '../data/DSA 2025 C07 Natural Language Processing_1.pdf', 'total_pages': 66, 'format': 'PDF 1.7', 'title': 'Data Science Applications', 'author': 'User', 'subject': 'Natural Language Processing', 'keywords': 'Chapter 7', 'moddate': '2024-12-18T04:49:02+00:00', 'trapped': '', 'modDate': 'D:20241218044902Z', 'creationDate': 'D:20241218044859Z', 'page': 20}, page_content='In this \\ncomponent of the TF-IDF score, a term’s weight is increased by the inverse of the number of \\ndocuments in which it appears.\\n\\nIn other words, common terms that appear in many documents in \\nthe corpus will be given a lower score.\\n\\nVideo 7.3 explains the intuition behind the TF-IDF method.\\n\\nVideo 7.3 – Term frequency-inverse document frequency \\n \\nRecord your video notes here \\n \\n \\n \\n \\n \\n \\nhttps://www.youtube.com/watch?v=OymqCnh-APA \\n(8 mins)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_res[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all PDFs and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x1b477e78070>,\n",
       " <langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x1b427d7c2e0>,\n",
       " <langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x1b477e7a0b0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/\"\n",
    "\n",
    "loaders = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loaders.append(PyMuPDFLoader(os.path.join(path, file)))\n",
    "\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 562, which is longer than the specified 500\n",
      "Created a chunk of size 611, which is longer than the specified 500\n",
      "Created a chunk of size 573, which is longer than the specified 500\n",
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 1238, which is longer than the specified 500\n",
      "Created a chunk of size 715, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 567, which is longer than the specified 500\n",
      "Created a chunk of size 510, which is longer than the specified 500\n",
      "Created a chunk of size 1415, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 754, which is longer than the specified 500\n",
      "Created a chunk of size 534, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 578, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 918, which is longer than the specified 500\n",
      "Created a chunk of size 777, which is longer than the specified 500\n",
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 845, which is longer than the specified 500\n",
      "Created a chunk of size 640, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 752, which is longer than the specified 500\n",
      "Created a chunk of size 761, which is longer than the specified 500\n",
      "Created a chunk of size 663, which is longer than the specified 500\n",
      "Created a chunk of size 727, which is longer than the specified 500\n",
      "Created a chunk of size 664, which is longer than the specified 500\n",
      "Created a chunk of size 709, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 664, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 1936, which is longer than the specified 500\n",
      "Created a chunk of size 540, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 708, which is longer than the specified 500\n",
      "Created a chunk of size 725, which is longer than the specified 500\n",
      "Created a chunk of size 1584, which is longer than the specified 500\n",
      "Created a chunk of size 645, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 636, which is longer than the specified 500\n",
      "Created a chunk of size 799, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# splits docs\n",
    "\n",
    "splits = nltk_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str=\"BAAI/bge-large-en-v1.5\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._get_embedding(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._get_embedding(text)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> list[float]:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True,\n",
    "                                truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Use CLS token\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = (\n",
    "    \"The sun sets in the evening, casting a warm orange glow across the horizon.\"\n",
    ")\n",
    "sentence2 = \"Twilight descends upon the land as the day draws to a close, painting the sky with hues of red and gold.\"\n",
    "sentence3 = \"Baby JJ crawled up the mattress to get his milk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0000000000000002)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding1)/np.sqrt(np.sum(np.square(embedding1)))/np.sqrt(np.sum(np.square(embedding1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings databases (also known as vector databases/stores) store embeddings and allow you to search by nearest neighbors rather than by substrings like a traditional database.  \n",
    "Here, Chroma is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"docs/chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixi\\AppData\\Local\\Temp\\ipykernel_25052\\1818982328.py:5: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits, embedding=embedding, persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does vectordb count tally with total splits?\n",
    "vectordb._collection.count() == len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing retrieval methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the documents and embeddings in the vectorstore, there are several ways to retrieve this information.  \n",
    "Here three methods are compared: `similarity_search`, `max_marginal_relevance_search` (MMR) and `ContextualCompressionRetriever`\n",
    "\n",
    "Similarity search: Selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/similarity\n",
    "\n",
    "MMR: Selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity.  \n",
    "It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]\n",
    "\n",
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Marginal Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMR penalizes the second text due to its similarity with the first, and instead returns the third text which is related yet different.  \n",
    "Let's try it on our docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question, k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_qn = \"Briefly explain the main clustering techniques\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using similarity search, the first two search results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This often runs counter to the aim of a clustering algorithm, which is to \\ndiscover the underlying structure of a dataset that was previously unknown.\\n\\nManual validation involves validation by a human expert, who reviews the clustering outcomes \\nand makes a subjective evaluation of whether they are ',\n",
       " 'As with most validation activities, it is likely that a combination of the above validation methods will \\nbe the most effective in determining the best model to adopt for the given purpose.\\n\\nThe optimal number of clusters will be typically determined by reference to multiple validation \\nmethods.',\n",
       " 'In unsupervised learning, response variables are often not available and not even \\nrelevant for the problem being solved.\\n\\nInstead, validation of clustering outcomes involves \\nassessing whether the characterisation of the data or simplification of the features is reasonable.\\n\\nThis means alternative ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_res = vectordb.similarity_search(docs_qn, k=3)\n",
    "[res.page_content[:300] for res in ss_res[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With mmr, there are no repeating results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This often runs counter to the aim of a clustering algorithm, which is to \\ndiscover the underlying structure of a dataset that was previously unknown.\\n\\nManual validation involves validation by a human',\n",
       " 'Once the distance measure for individual observations has been determined, these can be used \\nby ‘linkage’ methods to calculate the distance between two clusters.\\n\\nFour common linkage \\nmethods are: \\n•',\n",
       " 'Divisive Hierarchical Clustering \\nDivisive hierarchical clustering was introduced in Section 6.3.\\n\\nUnlike agglomerative hierarchical \\nclustering (see Section 6.3.3), which uses a bottom-up approach, d']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_res = vectordb.max_marginal_relevance_search(docs_qn, k=3, fetch_k=10)\n",
    "[res.page_content[:200] for res in mmr_res[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Divisive Hierarchical Clustering \\nDivisive hierarchical clustering was introduced in Section 6.3.\\n\\nUnlike agglomerative hierarchical \\nclustering (see Section 6.3.3), which uses a bottom-up approach, divisive hierarchical clustering \\nuses a top-down approach.\\n\\nThe top-down approach begins with all observations belonging to a \\nsingle cluster.\\n\\nAt each step of the algorithm, a cluster is split into two, and the number of clusters is \\nincreased by one.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a closer look at the third search result\n",
    "mmr_res[2].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I observed: A low `fetch_k` and a low `k` results in all results comes from the same document.  \n",
    "However, the other docs also contain information about Evergrande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/DSA 2025 C06 Unsupervised Learning.pdf', 38),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 28),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 34)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(res.metadata[\"source\"], res.metadata[\"page\"]) for res in mmr_res[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_res = vectordb.max_marginal_relevance_search(docs_qn, k=10, fetch_k=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing `k` and `fetch_k` seems to fix this. All three documents are now being cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/DSA 2025 C06 Unsupervised Learning.pdf', 17),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 19),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 25),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 28),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 34),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 34),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 35),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 38),\n",
       " ('../data/DSA 2025 C06 Unsupervised Learning.pdf', 56),\n",
       " ('../data/DSA 2025 C07 Natural Language Processing_1.pdf', 17)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(\n",
    "    [(res.metadata[\"source\"], res.metadata[\"page\"]) for res in mmr_res],\n",
    "    key=lambda x: (x[0], x[1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This often runs counter to the aim of a clustering algorithm, which is to \\ndiscover the underlying s',\n",
       " 'Once the distance measure for individual observations has been determined, these can be used \\nby ‘li',\n",
       " 'As was discussed in Chapter 5 (Classification and neural networks), the reaching of a local \\noptimum',\n",
       " 'Divisive Hierarchical Clustering \\nDivisive hierarchical clustering was introduced in Section 6.3.\\n\\nU',\n",
       " 'The justification for the clustering algorithm choices can include: \\n• \\nUse K-means when: \\n– \\nYou kn',\n",
       " 'Clustering can help to achieve specific outcomes, such as detecting fraud or identifying similar \\ngr',\n",
       " 'Medium.\\n\\nhttps://medium.com/predict/three-popular-clustering-\\nmethods-and-when-to-use-each-4227c80ba',\n",
       " 'Data Science Applications \\nChapter 6: Unsupervised Learning \\n \\n \\n \\n \\n \\n \\n \\n© December 2024 The Insti',\n",
       " 'These two effects can present computational challenges for running clustering algorithms \\nefficientl',\n",
       " 'An NLP task \\nis no different.\\n\\nThe same GLMs, neural networks, and k-means clustering models require']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.page_content[:100] for res in mmr_res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(\n",
    "    model=\"./models/mistral-7b-instruct-v0.1.Q4_0.gguf\",  # path to downloaded model\n",
    "    backend=\"llama\",  # required for GGUF\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 4: Create compressor\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixi\\AppData\\Local\\Temp\\ipykernel_25052\\4038341415.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(docs_qn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "- Clustering algorithm aims to discover previously unknown structure of dataset\n",
      "- Manual validation involves human expert reviewing clustering outcomes and making subjective evaluation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "- Clustering techniques\n",
      "- Validation activities\n",
      "- Determine the best model\n",
      "- Given purpose\n",
      "- Optimal number of clusters\n",
      "- Multiple validation methods\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "- unsupervised learning\n",
      "- response variables not available or relevant for problem being solved\n",
      "- validation of clustering outcomes involves assessing characterisation of data or simplification of features\n",
      "- alternative ways from those used to validate supervised learning outcomes are needed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "• Distance measure to use to calculate the distance between individual observations; and\n",
      "• Method of ‘linkage’ to use to calculate the distance between clusters, which may contain more than one observation.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(docs_qn)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vectordb.as_retriever()` calls Class VectorStoreRetriever which, by default, uses similarity search.  \n",
    "Hence we see repeated results once more. Let's examine the results using MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever_mmr = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "- Clustering algorithm aims to discover previously unknown structure of dataset\n",
      "- Manual validation involves human expert reviewing clustering outcomes and making subjective evaluation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "- distance measure for individual observations\n",
      "- linkage methods to calculate the distance between two clusters\n",
      "- four common linkage methods: complete linkage, single linkage, average linkage, centroid linkage\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Divisive hierarchical clustering was introduced in Section 6.3. Unlike agglomerative hierarchical clustering, which uses a bottom-up approach, divisive hierarchical clustering uses a top-down approach. The top-down approach begins with all observations belonging to a single cluster. At each step of the algorithm, a cluster is split into two, and the number of clusters is increased by one.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "K-means, number of clusters in advance, large datasets, spherical clusters, efficient algorithm, hierarchical clustering, smaller datasets, hierarchical understanding.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever_mmr.get_relevant_documents(docs_qn)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, MMR gives better results compared to similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval without vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_text = [d.page_content for d in docs]\n",
    "joined_docs_text = \" \".join(all_docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 562, which is longer than the specified 500\n",
      "Created a chunk of size 611, which is longer than the specified 500\n",
      "Created a chunk of size 573, which is longer than the specified 500\n",
      "Created a chunk of size 827, which is longer than the specified 500\n",
      "Created a chunk of size 1285, which is longer than the specified 500\n",
      "Created a chunk of size 715, which is longer than the specified 500\n",
      "Created a chunk of size 709, which is longer than the specified 500\n",
      "Created a chunk of size 535, which is longer than the specified 500\n",
      "Created a chunk of size 643, which is longer than the specified 500\n",
      "Created a chunk of size 567, which is longer than the specified 500\n",
      "Created a chunk of size 899, which is longer than the specified 500\n",
      "Created a chunk of size 510, which is longer than the specified 500\n",
      "Created a chunk of size 1649, which is longer than the specified 500\n",
      "Created a chunk of size 519, which is longer than the specified 500\n",
      "Created a chunk of size 674, which is longer than the specified 500\n",
      "Created a chunk of size 1114, which is longer than the specified 500\n",
      "Created a chunk of size 543, which is longer than the specified 500\n",
      "Created a chunk of size 518, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 754, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 662, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 578, which is longer than the specified 500\n",
      "Created a chunk of size 1176, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 598, which is longer than the specified 500\n",
      "Created a chunk of size 615, which is longer than the specified 500\n",
      "Created a chunk of size 526, which is longer than the specified 500\n",
      "Created a chunk of size 738, which is longer than the specified 500\n",
      "Created a chunk of size 507, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 918, which is longer than the specified 500\n",
      "Created a chunk of size 553, which is longer than the specified 500\n",
      "Created a chunk of size 777, which is longer than the specified 500\n",
      "Created a chunk of size 1474, which is longer than the specified 500\n",
      "Created a chunk of size 845, which is longer than the specified 500\n",
      "Created a chunk of size 630, which is longer than the specified 500\n",
      "Created a chunk of size 640, which is longer than the specified 500\n",
      "Created a chunk of size 733, which is longer than the specified 500\n",
      "Created a chunk of size 920, which is longer than the specified 500\n",
      "Created a chunk of size 761, which is longer than the specified 500\n",
      "Created a chunk of size 663, which is longer than the specified 500\n",
      "Created a chunk of size 727, which is longer than the specified 500\n",
      "Created a chunk of size 664, which is longer than the specified 500\n",
      "Created a chunk of size 709, which is longer than the specified 500\n",
      "Created a chunk of size 772, which is longer than the specified 500\n",
      "Created a chunk of size 664, which is longer than the specified 500\n",
      "Created a chunk of size 562, which is longer than the specified 500\n",
      "Created a chunk of size 881, which is longer than the specified 500\n",
      "Created a chunk of size 547, which is longer than the specified 500\n",
      "Created a chunk of size 895, which is longer than the specified 500\n",
      "Created a chunk of size 1147, which is longer than the specified 500\n",
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 1936, which is longer than the specified 500\n",
      "Created a chunk of size 1273, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 708, which is longer than the specified 500\n",
      "Created a chunk of size 1001, which is longer than the specified 500\n",
      "Created a chunk of size 725, which is longer than the specified 500\n",
      "Created a chunk of size 545, which is longer than the specified 500\n",
      "Created a chunk of size 1686, which is longer than the specified 500\n",
      "Created a chunk of size 539, which is longer than the specified 500\n",
      "Created a chunk of size 645, which is longer than the specified 500\n",
      "Created a chunk of size 585, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 636, which is longer than the specified 500\n",
      "Created a chunk of size 673, which is longer than the specified 500\n",
      "Created a chunk of size 799, which is longer than the specified 500\n",
      "Created a chunk of size 529, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "nltk_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "docs_splits = nltk_splitter.split_text(joined_docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_retriever = SVMRetriever.from_texts(\n",
    "    texts=docs_splits,\n",
    "    embeddings=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Discuss alternative clustering techniques \\n2.2.8.\\n\\nEvaluate a clustering algorithm using internal, external, and manual validation Data Science Applications \\nChapter 6: Unsupervised Learning \\n \\n \\n \\n \\n \\n \\n \\n© December 2024 The Institute of Actuaries of Australia \\nPage 4 of 63 \\n6.1.\\n\\nIntroduction \\nUnsupervised learning is a data science technique that looks for patterns in data.')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_svm = svm_retriever.get_relevant_documents(docs_qn)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works well but metadata is missing. There's probably is a way to include metadata, but that's out of the scope for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Discuss alternative clustering techniques \\n2.2.8.\\n\\nEvaluate a clustering algorithm using internal, external, and manual validation Data Science Applications \\nChapter 6: Unsupervised Learning \\n \\n \\n \\n \\n \\n \\n \\n© December 2024 The Institute of Actuaries of Australia \\nPage 4 of 63 \\n6.1.\\n\\nIntroduction \\nUnsupervised learning is a data science technique that looks for patterns in data.'),\n",
       " Document(metadata={}, page_content='The goal of clustering is to separate \\nobservations into homogenous groups or clusters based on their features such that the \\nobservations in each cluster are more like each other than observations in other clusters.\\n\\nClustering can help to achieve specific outcomes, such as detecting fraud or identifying similar \\ngroups of customers.\\n\\nClustering can also be used to summarise a complex situation or to produce \\neasy-to-understand narratives, which can be used by managers within a business.'),\n",
       " Document(metadata={}, page_content='Clustering \\ncan be used in fraud detection in one of two ways: \\n• \\nby identifying outliers that are dissimilar to other observations and do not align closely with any \\nof the clusters found in the dataset—these outliers are potential cases of fraud; or, conversely, \\n• \\nby identifying a cluster of observations, when all other observations appear to be more random \\nand not tightly bunched together—this method is described in Video 6.5.'),\n",
       " Document(metadata={}, page_content='As discussed in this chapter, one branch of unsupervised learning, clustering, involves \\nfinding subsets of a population that are like each other and different from other subsets.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixi\\AppData\\Local\\Temp\\ipykernel_25052\\2016500050.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": docs_qn})\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": docs_qn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Clustering is an unsupervised machine learning technique that involves grouping similar data points together based on their characteristics or features. There are several clustering techniques, including k-means, hierarchical clustering, density-based clustering, and fuzzy clustering. Each of these techniques has its own strengths and weaknesses, and the choice of technique depends on the nature of the data being analyzed and the goals of the analysis.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, \n",
    "just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible. The tone should be informative. Use bullet points. Cite the chapter reference\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_res = qa_chain(\n",
    "    {\"query\": \"Why is loss function important and what are the key optimisation methods?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The loss function is important because it represents the price paid for the inaccuracy of a model’s predictions, which is the measure by which a model is judged and optimised. A better classifier will produce a lower loss function score than a poorer model. Key optimisation methods include varying the learning rate over the training stage with a ‘learning rate scheduler’, plotting the calculated loss after each iteration of the gradient descent algorithm to ensure convergence, and using different batch sizes of data that is used in training.\n",
      "Reference: Chapter 5.2.2\n"
     ]
    }
   ],
   "source": [
    "print(qa_res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_refine = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Changed from \"refine\" to \"stuff\"\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `chain_type=\"stuff\"` is a lot more structured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The loss function is important because it represents the price paid for the inaccuracy of a model’s predictions, which is the measure by which a model is judged and optimised. A better classifier will produce a lower loss function score than a poorer model. There are several key optimisation methods that can be used to find the optimal values of the parameters of a neural network, including gradient descent with learning rate scheduling, stochastic gradient descent, Adam optimiser, and RMSprop optimiser.\n"
     ]
    }
   ],
   "source": [
    "refine_result = qa_chain_refine(\n",
    "    {\"query\": \"Why is loss function important and what are the key optimisation methods?\"}\n",
    ")\n",
    "print(refine_result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A neural network is a type of machine learning algorithm that uses artificial neurons to model complex relationships between inputs and outputs. It can be used for tasks such as classification, regression, and prediction. Neural networks are trained on large amounts of data and use backpropagation to adjust the weights of the connections between neurons in order to minimize error. They have been shown to be effective at solving a wide range of problems, but also come with risks such as reverse engineering, unintended classifications, model bias, and spurious learning.\n"
     ]
    }
   ],
   "source": [
    "refine_result = qa_chain_refine(\n",
    "    {\"query\": \"Describe at high-level what is a neural network.\"}\n",
    ")\n",
    "print(refine_result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
